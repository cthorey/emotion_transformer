---

title: Model

keywords: fastai
sidebar: home_sidebar

summary: "construction of the DoubleDistilBert model for the SemEval-2019 Task 3 dataset (contextual emotion detection in text)"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: 01_model.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
    
<div class="cell border-box-sizing code_cell rendered">

</div>
<div class="cell border-box-sizing code_cell rendered">

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Transformer-Sentence-Embeddings">Transformer Sentence Embeddings<a class="anchor-link" href="#Transformer-Sentence-Embeddings">&#182;</a></h2><p>First we create sentence embeddings for each utterance. We use a pretrained DistilBert model to obtain contextual word embeddings and then projecte the sum of the CLS token embedding and the mean of the second-to-last layer to a given <code>projection size</code>. Note that in order to feed batches into out model we need to temporarily flatten our <code>input_ids</code>, i.e. we get three times as many input sentences as the specified <code>batch_size</code>.</p>
<p>For more information on the (Distil)Bert models one can look at 
Jay Alammar's blog posts (<a href="https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/">A Visual Guide to Using BERT for the First Time</a> and <a href="https://jalammar.github.io/illustrated-bert/">The Illustrated BERT, ELMo, and co.</a>) where also the following illustration is taken from.</p>
<p><img src="./images/bert-distilbert-output-tensor-predictions.png" alt="DistilBert output"></p>
<p>Further references:</p>
<ul>
<li><a href="https://arxiv.org/abs/1910.01108">DistilBert paper</a> and <a href="https://medium.com/huggingface/distilbert-8cf3380435b5">blog post</a></li>
<li><a href="https://arxiv.org/abs/1810.04805">Original Bert (Bidirectional Encoder Representations from Transformers) paper</a></li>
<li><a href="https://pytorch.org/tutorials/beginner/pytorch_with_examples.html">tutorial for custom PyTorch modules</a></li>
<li><a href="https://huggingface.co/transformers/v2.3.0/index.html">Huggingface transformers documentation</a></li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="sentence_embeds_model" class="doc_header"><code>class</code> <code>sentence_embeds_model</code><a href="https://github.com/juliusberner/emotion_transformer/tree/master/emotion_transformer/model.py#L13" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>sentence_embeds_model</code>(<strong><code>projection_size</code></strong>, <strong><code>dropout</code></strong>=<em><code>0.1</code></em>) :: <code>Module</code></p>
</blockquote>
<p>instantiates the pretrained DistilBert model and the linear layer</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To illustrate the model let us import our dataloader.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">path</span> <span class="o">=</span> <span class="s1">&#39;data/clean_train.txt&#39;</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">max_seq_len</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">emo_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;others&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;sad&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;angry&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s1">&#39;happy&#39;</span><span class="p">:</span> <span class="mi">3</span><span class="p">}</span>
<span class="n">loader</span> <span class="o">=</span> <span class="n">dataloader</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">max_seq_len</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">emo_dict</span><span class="p">)</span>
<span class="n">input_ids</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">loader</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The DistilBert model outputs</p>
<ul>
<li>768-dimensional embeddings for each of the 'max_seq_len' tokens and each of the three utterances of the <code>batch_size</code> conversations and</li>
<li>a list of the hidden-states in all of the 6 DistilBert transformer layers (including the first embedding)</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">projection_size</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">embeds_model</span> <span class="o">=</span> <span class="n">sentence_embeds_model</span><span class="p">(</span><span class="n">projection_size</span><span class="p">)</span>

<span class="n">last_layer</span><span class="p">,</span> <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">embeds_model</span><span class="o">.</span><span class="n">transformer</span><span class="p">(</span><span class="n">input_ids</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">end_dim</span> <span class="o">=</span> <span class="mi">1</span><span class="p">),</span> <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">end_dim</span> <span class="o">=</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">input_embeds</span> <span class="o">=</span> <span class="n">embeds_model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">embeddings</span><span class="p">(</span><span class="n">input_ids</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">end_dim</span> <span class="o">=</span> <span class="mi">1</span><span class="p">))</span>

<span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">input_embeds</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">last_layer</span><span class="p">)</span>

<span class="nb">len</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">),</span> <span class="n">last_layer</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(7, torch.Size([15, 10, 768]))</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let us now create sentence embeddings (we put the model in evaluation mode to deactivate dropout for later consistency checks). Note that the forward method of the model reshapes the output again back to the shape of the corresponding <code>input_ids</code>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">embeds_model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="k">assert</span><span class="p">(</span><span class="n">embeds_model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">layer</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">dropout</span><span class="o">.</span><span class="n">training</span> <span class="o">==</span> <span class="kc">False</span><span class="p">)</span>

<span class="n">sentence_embeds</span> <span class="o">=</span> <span class="n">embeds_model</span><span class="p">(</span><span class="n">input_ids</span> <span class="o">=</span> <span class="n">input_ids</span><span class="p">,</span> <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">assert</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span> <span class="o">==</span> <span class="n">sentence_embeds</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span>
<span class="k">assert</span> <span class="n">sentence_embeds</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">embeds_model</span><span class="o">.</span><span class="n">projection_size</span>
<span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">sentence_embeds</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(torch.Size([5, 3, 10]), torch.Size([5, 3, 100]))</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We also check if the <code>layerwise_lr</code> method outputs all model parameters.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">count</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="n">embeds_model</span><span class="o">.</span><span class="n">layerwise_lr</span><span class="p">(</span><span class="mf">2.0e-5</span><span class="p">,</span><span class="mf">0.95</span><span class="p">):</span>
    <span class="n">count</span> <span class="o">+=</span> <span class="nb">len</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">group</span><span class="p">[</span><span class="s1">&#39;params&#39;</span><span class="p">]))</span>

<span class="k">assert</span> <span class="n">count</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">embeds_model</span><span class="o">.</span><span class="n">parameters</span><span class="p">()))</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Context-Transformer-and-Classification">Context Transformer and Classification<a class="anchor-link" href="#Context-Transformer-and-Classification">&#182;</a></h2><p>Next we use another transformer model to create contextual sentence embeddings, i.e. we model that a conversation consists of three utterances. This is partly motivated by the <a href="https://arxiv.org/abs/1903.10318">BERTSUM paper</a>.</p>
<p>Moreover, we add a classification model for the emotion of the last utterance where we augment the loss by a binary loss due to the unbalanced data.</p>
<p>Note that for our convenience we use</p>
<ul>
<li>a (not pre-trained) DistilBertForSequenceClassification and flip the order of the utterances as the first input embedding gets classified by default</li>
<li>only one attention head, see also the paper <a href="https://arxiv.org/abs/1905.10650">Are Sixteen Heads Really Better than One?</a>.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="context_classifier_model" class="doc_header"><code>class</code> <code>context_classifier_model</code><a href="https://github.com/juliusberner/emotion_transformer/tree/master/emotion_transformer/model.py#L59" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>context_classifier_model</code>(<strong><code>hidden_size</code></strong>, <strong><code>n_layers</code></strong>, <strong><code>emo_dict</code></strong>, <strong><code>dropout</code></strong>=<em><code>0.1</code></em>) :: <code>Module</code></p>
</blockquote>
<p>instantiates the DisitlBertForSequenceClassification model, the position embeddings of the utterances,
and the binary loss function</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let us initiate a the <a href="model.html#context_classifier_model"><code>context_classifier_model</code></a> with the corresponding <code>projection_size</code> of the sentence embedding model</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">embeds_model</span><span class="o">.</span><span class="n">projection_size</span>
<span class="n">n_layers</span> <span class="o">=</span> <span class="mi">2</span>

<span class="n">classifier</span> <span class="o">=</span> <span class="n">context_classifier_model</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">n_layers</span><span class="p">,</span> <span class="n">emo_dict</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>and do some basic checks.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">classifier</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="k">assert</span><span class="p">(</span><span class="n">classifier</span><span class="o">.</span><span class="n">context_transformer</span><span class="o">.</span><span class="n">distilbert</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">layer</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">dropout</span><span class="o">.</span><span class="n">training</span> <span class="o">==</span> <span class="kc">False</span><span class="p">)</span>

<span class="n">loss</span><span class="p">,</span> <span class="n">logits</span> <span class="o">=</span> <span class="n">classifier</span><span class="p">(</span><span class="n">sentence_embeds</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">logits</span> <span class="o">==</span> <span class="n">classifier</span><span class="p">(</span><span class="n">sentence_embeds</span><span class="p">))</span>
<span class="k">assert</span> <span class="n">loss</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span> <span class="o">+</span> <span class="n">classifier</span><span class="o">.</span><span class="n">bin_loss</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>

<span class="n">loss</span><span class="p">,</span> <span class="n">logits</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(tensor(2.0534, grad_fn=&lt;AddBackward0&gt;),
 tensor([[-0.0060,  0.0565, -0.0573, -0.0431],
         [-0.0034,  0.0541, -0.0548, -0.0447],
         [-0.0109,  0.0533, -0.0560, -0.0412],
         [ 0.0047,  0.0604, -0.0612, -0.0435],
         [-0.0031,  0.0494, -0.0585, -0.0466]], grad_fn=&lt;AddmmBackward&gt;))</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Finally, for our main consistency check we compute the gradient of the loss w.r.t. to the input embeddings.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">input_embeds</span> <span class="o">=</span> <span class="n">input_embeds</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">sentence_embeds_check</span> <span class="o">=</span> <span class="n">embeds_model</span><span class="p">(</span><span class="n">input_embeds</span> <span class="o">=</span> <span class="n">input_embeds</span><span class="p">,</span> <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="p">)</span>
<span class="n">logits_check</span> <span class="o">=</span> <span class="n">classifier</span><span class="p">(</span><span class="n">sentence_embeds_check</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">logits</span> <span class="o">==</span> <span class="n">logits_check</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">logits_check</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="n">input_embeds</span><span class="o">.</span><span class="n">grad</span><span class="p">[:,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 7.2630e-07, 1.2209e-06, 1.0843e-04,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00])</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>As anticipated, we see that only the fourth, fifth, and sixth input embedding effect the second prediction.
These correspond to the second conversation:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">input_embeds</span><span class="p">[</span><span class="mi">3</span><span class="p">:</span><span class="mi">6</span><span class="p">]</span> <span class="o">==</span> <span class="n">embeds_model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">embeddings</span><span class="p">(</span><span class="n">input_ids</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Metrics">Metrics<a class="anchor-link" href="#Metrics">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Lastly, we define the metrics for the evaluation of our model according to the <a href="https://www.aclweb.org/anthology/S19-2005/">SemEval-2019 Task 3 challenge</a>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="metrics" class="doc_header"><code>metrics</code><a href="https://github.com/juliusberner/emotion_transformer/tree/master/emotion_transformer/model.py#L112" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>metrics</code>(<strong><code>loss</code></strong>, <strong><code>logits</code></strong>, <strong><code>labels</code></strong>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="f1_score" class="doc_header"><code>f1_score</code><a href="https://github.com/juliusberner/emotion_transformer/tree/master/emotion_transformer/model.py#L124" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>f1_score</code>(<strong><code>tp</code></strong>, <strong><code>fp</code></strong>, <strong><code>fn</code></strong>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">metric</span> <span class="o">=</span> <span class="n">metrics</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
<span class="n">metric</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>{&#39;val_loss&#39;: tensor(2.0534, grad_fn=&lt;AddBackward0&gt;),
 &#39;val_acc&#39;: tensor(0.4000),
 &#39;tp&#39;: tensor(2.),
 &#39;fp&#39;: tensor(3.),
 &#39;fn&#39;: tensor(1.)}</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">f1_score</span><span class="p">(</span><span class="n">metric</span><span class="p">[</span><span class="s1">&#39;tp&#39;</span><span class="p">],</span> <span class="n">metric</span><span class="p">[</span><span class="s1">&#39;fp&#39;</span><span class="p">],</span> <span class="n">metric</span><span class="p">[</span><span class="s1">&#39;fn&#39;</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>{&#39;precision&#39;: tensor(0.4000),
 &#39;recall&#39;: tensor(0.6667),
 &#39;f1_score&#39;: tensor(0.5000)}</pre>
</div>

</div>

</div>
</div>

</div>
</div>
 

